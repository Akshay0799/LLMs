{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_community langchain_chroma\n",
    "!pip install langchainhub\n",
    "!pip install bs4\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langsmith in d:\\llm\\llm_env\\lib\\site-packages (0.1.86)\n",
      "Collecting langsmith\n",
      "  Obtaining dependency information for langsmith from https://files.pythonhosted.org/packages/1e/b9/b29cddfd6f4b5ea3276a2552ce0fefa38d30d5b63f43ed6a440903d6ca3e/langsmith-0.1.88-py3-none-any.whl.metadata\n",
      "  Using cached langsmith-0.1.88-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\llm\\llm_env\\lib\\site-packages (from langsmith) (3.10.6)\n",
      "Requirement already satisfied: pydantic<3,>=1 in d:\\llm\\llm_env\\lib\\site-packages (from langsmith) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\llm\\llm_env\\lib\\site-packages (from langsmith) (2.32.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\llm\\llm_env\\lib\\site-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in d:\\llm\\llm_env\\lib\\site-packages (from pydantic<3,>=1->langsmith) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in d:\\llm\\llm_env\\lib\\site-packages (from pydantic<3,>=1->langsmith) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\llm\\llm_env\\lib\\site-packages (from requests<3,>=2->langsmith) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\llm\\llm_env\\lib\\site-packages (from requests<3,>=2->langsmith) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\llm\\llm_env\\lib\\site-packages (from requests<3,>=2->langsmith) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\llm\\llm_env\\lib\\site-packages (from requests<3,>=2->langsmith) (2024.7.4)\n",
      "Using cached langsmith-0.1.88-py3-none-any.whl (134 kB)\n",
      "Installing collected packages: langsmith\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.1.86\n",
      "    Uninstalling langsmith-0.1.86:\n",
      "      Successfully uninstalled langsmith-0.1.86\n",
      "Successfully installed langsmith-0.1.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 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\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open AI access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sk-proj-UWuZLnHsuhARaSGb1rf6T3BlbkFJ0sXzlwRdI5YpAMXGjLcu\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google LLM access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-google-vertexai --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM\\llm_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyCkDUFgzBVgfn8f0eBRzAuAPXXWyzzN1FE\"\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "# llm = ChatVertexAI(model=\"gemini-pro\", project_id=\"gen-lang-client-0221906983\")\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "# llm = genai.GenerativeModel('gemini-1.5-pro')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "d:\\LLM\\llm_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller, more manageable steps. \\nThis can be done through simple prompting or by using task-specific instructions. \\nTask decomposition helps LLMs utilize more test-time computation and shed light on the model’s thinking process.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "from langsmith import traceable\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "# vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\"))\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "@traceable\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")\n",
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAVILY_API_KEY = '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'\n",
    "os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG - Agentic Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing in VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=splits, embedding=SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorDb Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type = \"similarity_score_threshold\", search_kwargs={\"k\": 3, \"score_threshold\": 0.3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Retrieval Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Data model for LLM output format\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "# LLM for grading\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "# Prompt template for grading\n",
    "SYS_PROMPT = \"\"\"You are an expert grader assessing relevance of a retrieved document to a user question.\n",
    "                Follow these instructions for grading:\n",
    "                  - If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "                  - Your grade should be either 'yes' or 'no' to indicate whether the document is relevant to the question or not.\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"human\", \"\"\"Retrieved document:\n",
    "                     {document}\n",
    "                     User question:\n",
    "                     {question}\n",
    "                  \"\"\"),\n",
    "    ]\n",
    ")\n",
    "# Build grader chain\n",
    "doc_grader = (grade_prompt\n",
    "                  |\n",
    "              structured_llm_grader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "GRADE: binary_score='yes'\n",
      "\n",
      "Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\n",
      "Component Two: Memory#\n",
      "(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\n",
      "Types of Memory#\n",
      "Memory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\n",
      "\n",
      "\n",
      "Sensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).\n",
      "GRADE: binary_score='yes'\n",
      "\n",
      "Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\n",
      "\n",
      "\n",
      "Long-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\n",
      "\n",
      "Explicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\n",
      "Implicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fig. 8. Categorization of human memory.\n",
      "We can roughly consider the following mappings:\n",
      "GRADE: binary_score='yes'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the different types of memory?\"\n",
    "top3_docs = retriever.invoke(query)\n",
    "for doc in top3_docs:\n",
    "    print(doc.page_content)\n",
    "    print('GRADE:', doc_grader.invoke({\"question\": query, \n",
    "                                       \"document\": doc.page_content}))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "# Create RAG prompt for response generation\n",
    "prompt = \"\"\"You are an assistant for question-answering tasks.\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If no context is present or if you don't know the answer, just say that you don't know the answer.\n",
    "            Do not make up the answer unless it is there in the provided context.\n",
    "            Give a detailed answer and to the point answer with regard to the question.\n",
    "            Question:\n",
    "            {question}\n",
    "            Context:\n",
    "            {context}\n",
    "            Answer:\n",
    "         \"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(prompt)\n",
    "# Initialize connection with LLM\n",
    "chatgpt = llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "# Used for separating context docs with new lines\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "# create QA RAG chain\n",
    "qa_rag_chain = (\n",
    "    {\n",
    "        \"context\": (itemgetter('context')\n",
    "                        |\n",
    "                    RunnableLambda(format_docs)),\n",
    "        \"question\": itemgetter('question')\n",
    "    }\n",
    "      |\n",
    "    prompt_template\n",
    "      |\n",
    "    chatgpt\n",
    "      |\n",
    "    StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It is often used in conjunction with Chain of Thought (CoT) prompting, where a large language model is instructed to \"think step by step\" to solve a problem. This decomposition makes the task easier to manage and provides insight into the model's reasoning process. \n",
      "\n",
      "There are three main ways to perform task decomposition:\n",
      "\n",
      "1. **Simple Prompting:** Using prompts like \"Steps for XYZ.\\n1.\" or \"What are the subgoals for achieving XYZ?\" \n",
      "2. **Task-Specific Instructions:** Providing specific instructions related to the task, such as \"Write a story outline\" for a novel writing task.\n",
      "3. **Human Input:** Directly involving humans in the decomposition process. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is Task Decomposition?\"\n",
    "top3_docs = retriever.invoke(query)\n",
    "result = qa_rag_chain.invoke(\n",
    "    {\"context\": top3_docs, \"question\": query}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top3_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Rephraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = \"\"\"Act as a question re-writer and perform the following task:\n",
    "                 - Convert the following input question to a better version that is optimized for web search.\n",
    "                 - When re-writing, look at the input question and try to reason about the underlying semantic intent / meaning.\n",
    "             \"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", SYS_PROMPT),\n",
    "        (\"human\", \"\"\"Here is the initial question:\n",
    "                     {question}\n",
    "                     Formulate an improved question.\n",
    "                  \"\"\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create rephraser chain\n",
    "question_rewriter = (re_write_prompt\n",
    "                        |\n",
    "                       llm\n",
    "                        |\n",
    "                     StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Improved Question:** **Who is the projected winner of the 2024 Champions League?** \\n\\n**Reasoning:**\\n\\n* **Future Event:** The original question asks about the winner of a tournament in the future (2024). As of today (2023), this information is not available.\\n* **Intent:** The user likely wants to know predictions, betting odds, or early favorites for the 2024 Champions League title. \\n* **Clarity:** By using \"projected winner,\" we clarify that the question seeks speculation or forecasts rather than a definitive answer. \\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who won the champions league in 2024?\"\n",
    "question_rewriter.invoke({\"question\": query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tv_search = TavilySearchResults(max_results=3, search_depth='advanced',max_tokens=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agentic Workflow with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM response generation\n",
    "        web_search_needed: flag of whether to add web search - yes or no\n",
    "        documents: list of context documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search_needed: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents - that contains retrieved context documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVAL FROM VECTOR DB---\")\n",
    "    question = state[\"question\"]\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question\n",
    "    by using an LLM Grader.\n",
    "    If any document are not relevant to question or documents are empty - Web Search needs to be done\n",
    "    If all documents are relevant to question - Web Search is not needed\n",
    "    Helps filtering out irrelevant documents\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search_needed = \"No\"\n",
    "    if documents:\n",
    "        for d in documents:\n",
    "            score = doc_grader.invoke(\n",
    "                {\"question\": question, \"document\": d.page_content}\n",
    "            )\n",
    "            grade = score.binary_score\n",
    "            if grade == \"yes\":\n",
    "                print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "                filtered_docs.append(d)\n",
    "            else:\n",
    "                print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "                web_search_needed = \"Yes\"\n",
    "                continue\n",
    "    else:\n",
    "        print(\"---NO DOCUMENTS RETRIEVED---\")\n",
    "        web_search_needed = \"Yes\"\n",
    "    return {\"documents\": filtered_docs, \"question\": question, \n",
    "            \"web_search_needed\": web_search_needed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(state):\n",
    "    \"\"\"\n",
    "    Rewrite the query to produce a better question.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased or re-written question\n",
    "    \"\"\"\n",
    "    print(\"---REWRITE QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-written question.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # Web search\n",
    "    docs = tv_search.invoke(question)\n",
    "    web_results = \"\\n\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "    documents.append(web_results)\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(state):\n",
    "    \"\"\"\n",
    "    Generate answer from context document using LLM\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    # RAG generation\n",
    "    generation = qa_rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \n",
    "            \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    web_search_needed = state[\"web_search_needed\"]\n",
    "    if web_search_needed == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: SOME or ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUERY---\")\n",
    "        return \"rewrite_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE RESPONSE---\")\n",
    "        return \"generate_answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agentic Graph with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "agentic_rag = StateGraph(GraphState)\n",
    "# Define the nodes\n",
    "agentic_rag.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "agentic_rag.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "agentic_rag.add_node(\"rewrite_query\", rewrite_query)  # transform_query\n",
    "agentic_rag.add_node(\"web_search\", web_search)  # web search\n",
    "agentic_rag.add_node(\"generate_answer\", generate_answer)  # generate answer\n",
    "# Build graph\n",
    "agentic_rag.set_entry_point(\"retrieve\")\n",
    "agentic_rag.add_edge(\"retrieve\", \"grade_documents\")\n",
    "agentic_rag.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\"rewrite_query\": \"rewrite_query\", \"generate_answer\": \"generate_answer\"},\n",
    ")\n",
    "agentic_rag.add_edge(\"rewrite_query\", \"web_search\")\n",
    "agentic_rag.add_edge(\"web_search\", \"generate_answer\")\n",
    "agentic_rag.add_edge(\"generate_answer\", END)\n",
    "# Compile\n",
    "agentic_rag = agentic_rag.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, Markdown\n",
    "display(Image(agentic_rag.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
