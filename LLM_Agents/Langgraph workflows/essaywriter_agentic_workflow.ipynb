{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "# model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLAN_PROMPT = \"\"\"You are an expert composer tasked with writing a high level outline of an document. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the document along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an document assistant tasked with writing excellent 5-paragraph documents.\\\n",
    "Generate the best document possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an document submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following document. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only create 3 queries max.\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only create 3 queries max.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3579: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_result(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_result(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x127dfa26a50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wentity = StateGraph(AgentState)\n",
    "\n",
    "Wentity.add_node(\"planner\", plan_node)\n",
    "Wentity.add_node(\"create\", generation_node)\n",
    "Wentity.add_node(\"reflect\", reflection_node)\n",
    "Wentity.add_node(\"research_plan\", research_plan_node)\n",
    "Wentity.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "Wentity.set_entry_point(\"planner\")\n",
    "\n",
    "Wentity.add_conditional_edges(\n",
    "    \"create\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "Wentity.add_edge(\"planner\", \"research_plan\")\n",
    "Wentity.add_edge(\"research_plan\", \"create\")\n",
    "\n",
    "Wentity.add_edge(\"reflect\", \"research_critique\")\n",
    "Wentity.add_edge(\"research_critique\", \"create\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': 'Okay, here\\'s a high-level essay outline on the hype surrounding Generative AI, designed to provide a structured and insightful exploration of the topic:\\n\\n**Essay Title (Example):** Riding the Wave: Understanding and Navigating the Hype Around Generative AI\\n\\n**I. Introduction**\\n\\n*   **Hook:** Start with a compelling statistic or anecdote illustrating the rapid rise and widespread attention given to generative AI (e.g., investment figures, viral AI-generated content, or a quote from a prominent figure).\\n*   **Background:** Briefly define generative AI and provide a few key examples (e.g., DALL-E, GPT, Midjourney, Stable Diffusion). Explain its core capabilities in simple terms.\\n*   **Thesis Statement:**  Clearly state the essay\\'s main argument.  For example: \"While generative AI holds immense potential, the current hype surrounding it is fueled by a combination of genuine technological advancements, exaggerated expectations, and strategic marketing, necessitating a balanced and critical perspective.\"\\n*   **Roadmap:** Briefly outline the main points that will be discussed in the essay.\\n\\n**II. The Foundations of the Hype: Genuine Advancements**\\n\\n*   **Technological Breakthroughs:**\\n    *   Discuss the significant advancements in deep learning, particularly transformer models, that have enabled generative AI\\'s capabilities.\\n    *   Explain how these models learn from vast datasets to generate novel content.\\n    *   Provide specific examples of how these advancements translate into tangible improvements (e.g., higher quality image generation, more coherent text generation).\\n*   **Increased Accessibility:**\\n    *   Explain how user-friendly interfaces and cloud-based platforms have made generative AI tools accessible to a wider audience, including non-technical users.\\n    *   Discuss the impact of open-source models and pre-trained models on lowering the barrier to entry.\\n*   **Real-World Applications:**\\n    *   Highlight successful and promising applications of generative AI across various industries (e.g., marketing, design, content creation, drug discovery).\\n    *   Provide concrete examples of how generative AI is being used to solve real-world problems or create new opportunities.\\n\\n**III. Fueling the Fire: Exaggerated Expectations and Misconceptions**\\n\\n*   **The \"AI Revolution\" Narrative:**\\n    *   Analyze how media coverage and marketing campaigns often portray generative AI as a revolutionary force that will fundamentally transform society and the economy.\\n    *   Discuss the potential for overblown claims and unrealistic expectations regarding AI\\'s capabilities and impact.\\n*   **Misunderstanding of Limitations:**\\n    *   Explain the limitations of current generative AI models, such as their reliance on large datasets, their susceptibility to biases, and their lack of true understanding or consciousness.\\n    *   Provide examples of how these limitations can lead to flawed or problematic outputs.\\n*   **Fear of Job Displacement:**\\n    *   Address the concerns about generative AI automating jobs and displacing human workers.\\n    *   Discuss the potential for both job creation and job displacement, and the need for proactive measures to mitigate negative impacts.\\n\\n**IV. The Marketing Machine: Strategic Promotion and Investment**\\n\\n*   **Venture Capital and Investment:**\\n    *   Discuss the significant influx of venture capital into generative AI startups and the role of investors in driving hype and promoting the technology.\\n    *   Analyze the potential for conflicts of interest and the pressure to deliver rapid returns on investment.\\n*   **Big Tech\\'s Influence:**\\n    *   Examine how major technology companies are investing heavily in generative AI and using it to enhance their existing products and services.\\n    *   Discuss the potential for these companies to shape the narrative around generative AI and control its development and deployment.\\n*   **The \"Next Big Thing\" Syndrome:**\\n    *   Explain how the tech industry\\'s tendency to latch onto the \"next big thing\" can contribute to hype and create a sense of urgency around generative AI.\\n    *   Discuss the potential for this hype to overshadow more incremental but equally important technological advancements.\\n\\n**V. Navigating the Hype: A Balanced Perspective**\\n\\n*   **Critical Evaluation:**\\n    *   Emphasize the importance of critically evaluating claims about generative AI and separating hype from reality.\\n    *   Encourage readers to question the assumptions and biases underlying these claims.\\n*   **Focus on Practical Applications:**\\n    *   Advocate for a focus on developing and deploying generative AI in ways that address real-world problems and create tangible value.\\n    *   Highlight the importance of ethical considerations and responsible development practices.\\n*   **Long-Term Vision:**\\n    *   Encourage a long-term perspective on generative AI, recognizing that it is still in its early stages of development and that its full potential remains to be seen.\\n    *   Discuss the need for ongoing research and development to address the limitations of current models and unlock new possibilities.\\n\\n**VI. Conclusion**\\n\\n*   **Restate Thesis:** Summarize the main argument of the essay, emphasizing the need for a balanced and critical perspective on the hype surrounding generative AI.\\n*   **Summarize Key Points:** Briefly recap the main points discussed in the essay.\\n*   **Concluding Thought:** Offer a final thought or call to action, encouraging readers to engage with generative AI in a thoughtful and responsible manner.  Perhaps suggest future areas of focus or potential benefits if approached correctly.\\n\\n**Notes/Instructions:**\\n\\n*   **Evidence:**  Support your arguments with credible sources, including academic research, industry reports, news articles, and expert opinions.\\n*   **Examples:**  Use concrete examples to illustrate your points and make your essay more engaging.\\n*   **Clarity:**  Write in a clear and concise style, avoiding jargon and technical terms that may not be familiar to a general audience.\\n*   **Balance:**  Strive for a balanced perspective, acknowledging both the potential benefits and the potential risks of generative AI.\\n*   **Originality:**  Offer your own insights and analysis, rather than simply summarizing existing information.\\n*   **Target Audience:** Consider your target audience when writing the essay. Are you writing for a general audience, a technical audience, or a business audience? Adjust your language and level of detail accordingly.\\n*   **Ethical Considerations:** Throughout the essay, consider the ethical implications of generative AI, such as bias, fairness, and privacy.\\n\\nThis outline should provide a solid framework for your essay. Good luck!'}}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\essaywriter_agentic_workflow.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m graph \u001b[39m=\u001b[39m Wagent\u001b[39m.\u001b[39mcompile(checkpointer\u001b[39m=\u001b[39mcheckpointer)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m thread \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mconfigurable\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mthread_id\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m}}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m graph\u001b[39m.\u001b[39;49mstream({\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtask\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHyper around Generative AI\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mmax_revisions\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrevision_number\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m }, thread):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(s)\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1724\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1718\u001b[0m     \u001b[39m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m     \u001b[39m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m     \u001b[39m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1721\u001b[0m     \u001b[39m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m     \u001b[39m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m     \u001b[39mwhile\u001b[39;00m loop\u001b[39m.\u001b[39mtick(input_keys\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_channels):\n\u001b[1;32m-> 1724\u001b[0m         \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m runner\u001b[39m.\u001b[39;49mtick(\n\u001b[0;32m   1725\u001b[0m             loop\u001b[39m.\u001b[39;49mtasks\u001b[39m.\u001b[39;49mvalues(),\n\u001b[0;32m   1726\u001b[0m             timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_timeout,\n\u001b[0;32m   1727\u001b[0m             retry_policy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretry_policy,\n\u001b[0;32m   1728\u001b[0m             get_waiter\u001b[39m=\u001b[39;49mget_waiter,\n\u001b[0;32m   1729\u001b[0m         ):\n\u001b[0;32m   1730\u001b[0m             \u001b[39m# emit output\u001b[39;49;00m\n\u001b[0;32m   1731\u001b[0m             \u001b[39myield from\u001b[39;49;00m output()\n\u001b[0;32m   1732\u001b[0m \u001b[39m# emit output\u001b[39;00m\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[39m=\u001b[39m tasks[\u001b[39m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     run_with_retry(\n\u001b[0;32m    231\u001b[0m         t,\n\u001b[0;32m    232\u001b[0m         retry_policy,\n\u001b[0;32m    233\u001b[0m         configurable\u001b[39m=\u001b[39;49m{\n\u001b[0;32m    234\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[0;32m    235\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[0;32m    236\u001b[0m         },\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    238\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommit(t, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[39m.\u001b[39mwrites\u001b[39m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[39m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[39mreturn\u001b[39;00m task\u001b[39m.\u001b[39;49mproc\u001b[39m.\u001b[39;49minvoke(task\u001b[39m.\u001b[39;49minput, config)\n\u001b[0;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m ParentCommand \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\utils\\runnable.py:506\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m config \u001b[39m=\u001b[39m patch_config(\n\u001b[0;32m    503\u001b[0m     config, callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mseq:step:\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    504\u001b[0m )\n\u001b[0;32m    505\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 506\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\u001b[39minput\u001b[39;49m, config, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39minvoke(\u001b[39minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\utils\\runnable.py:270\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    269\u001b[0m     context\u001b[39m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m--> 270\u001b[0m     ret \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    271\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(ret, Runnable) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecurse:\n\u001b[0;32m    272\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\u001b[39m.\u001b[39minvoke(\u001b[39minput\u001b[39m, config)\n",
      "\u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\essaywriter_agentic_workflow.ipynb Cell 15\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mresearch_plan_node\u001b[39m(state: AgentState):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     queries \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mwith_structured_output(Queries)\u001b[39m.\u001b[39minvoke([\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         SystemMessage(content\u001b[39m=\u001b[39mRESEARCH_PLAN_PROMPT),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         HumanMessage(content\u001b[39m=\u001b[39mstate[\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     ])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     content \u001b[39m=\u001b[39m state[\u001b[39m'\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39mor\u001b[39;00m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m q \u001b[39min\u001b[39;00m queries\u001b[39m.\u001b[39mqueries:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         response \u001b[39m=\u001b[39m tavily\u001b[39m.\u001b[39msearch(query\u001b[39m=\u001b[39mq, max_results\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'content'",
      "\u001b[0mDuring task with name 'research_plan' and id '4c476af3-d49e-5310-8026-fcbb735b81d6'"
     ]
    }
   ],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    graph = Wentity.compile(checkpointer=checkpointer)\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for s in graph.stream({\n",
    "        'task': \"Hyper around Generative AI\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "Cannot operate on a closed database.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\essaywriter_agentic_workflow.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m thread \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mconfigurable\u001b[39m\u001b[39m\"\u001b[39m: {\u001b[39m\"\u001b[39m\u001b[39mthread_id\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m}}\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m s \u001b[39min\u001b[39;49;00m graph\u001b[39m.\u001b[39;49mstream({\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m'\u001b[39;49m\u001b[39mtask\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mwhat is the difference between langchain and langsmith\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mmax_revisions\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrevision_number\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }, thread):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/LLM/DeepLearnin.ai_Agents/essaywriter_agentic_workflow.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(s)\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1668\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcustom\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m stream_modes:\n\u001b[0;32m   1665\u001b[0m     config[CONF][CONFIG_KEY_STREAM_WRITER] \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m c: stream\u001b[39m.\u001b[39mput(\n\u001b[0;32m   1666\u001b[0m         ((), \u001b[39m\"\u001b[39m\u001b[39mcustom\u001b[39m\u001b[39m\"\u001b[39m, c)\n\u001b[0;32m   1667\u001b[0m     )\n\u001b[1;32m-> 1668\u001b[0m \u001b[39mwith\u001b[39;49;00m SyncPregelLoop(\n\u001b[0;32m   1669\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m   1670\u001b[0m     stream\u001b[39m=\u001b[39;49mStreamProtocol(stream\u001b[39m.\u001b[39;49mput, stream_modes),\n\u001b[0;32m   1671\u001b[0m     config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m   1672\u001b[0m     store\u001b[39m=\u001b[39;49mstore,\n\u001b[0;32m   1673\u001b[0m     checkpointer\u001b[39m=\u001b[39;49mcheckpointer,\n\u001b[0;32m   1674\u001b[0m     nodes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnodes,\n\u001b[0;32m   1675\u001b[0m     specs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchannels,\n\u001b[0;32m   1676\u001b[0m     output_keys\u001b[39m=\u001b[39;49moutput_keys,\n\u001b[0;32m   1677\u001b[0m     stream_keys\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream_channels_asis,\n\u001b[0;32m   1678\u001b[0m     interrupt_before\u001b[39m=\u001b[39;49minterrupt_before_,\n\u001b[0;32m   1679\u001b[0m     interrupt_after\u001b[39m=\u001b[39;49minterrupt_after_,\n\u001b[0;32m   1680\u001b[0m     manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[0;32m   1681\u001b[0m     debug\u001b[39m=\u001b[39;49mdebug,\n\u001b[0;32m   1682\u001b[0m ) \u001b[39mas\u001b[39;49;00m loop:\n\u001b[0;32m   1683\u001b[0m     \u001b[39m# create runner\u001b[39;49;00m\n\u001b[0;32m   1684\u001b[0m     runner \u001b[39m=\u001b[39;49m PregelRunner(\n\u001b[0;32m   1685\u001b[0m         submit\u001b[39m=\u001b[39;49mloop\u001b[39m.\u001b[39;49msubmit,\n\u001b[0;32m   1686\u001b[0m         put_writes\u001b[39m=\u001b[39;49mloop\u001b[39m.\u001b[39;49mput_writes,\n\u001b[0;32m   1687\u001b[0m         schedule_task\u001b[39m=\u001b[39;49mloop\u001b[39m.\u001b[39;49maccept_push,\n\u001b[0;32m   1688\u001b[0m         node_finished\u001b[39m=\u001b[39;49mconfig[CONF]\u001b[39m.\u001b[39;49mget(CONFIG_KEY_NODE_FINISHED),\n\u001b[0;32m   1689\u001b[0m     )\n\u001b[0;32m   1690\u001b[0m     \u001b[39m# enable subgraph streaming\u001b[39;49;00m\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\pregel\\loop.py:915\u001b[0m, in \u001b[0;36mSyncPregelLoop.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[39mraise\u001b[39;00m CheckpointNotLatest\n\u001b[0;32m    914\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheckpointer:\n\u001b[1;32m--> 915\u001b[0m     saved \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpointer\u001b[39m.\u001b[39;49mget_tuple(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_config)\n\u001b[0;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     saved \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\checkpoint\\sqlite\\__init__.py:213\u001b[0m, in \u001b[0;36mSqliteSaver.get_tuple\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get a checkpoint tuple from the database.\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[39mThis method retrieves a checkpoint tuple from the SQLite database based on the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39m    CheckpointTuple(...)\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    212\u001b[0m checkpoint_ns \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39mconfigurable\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcheckpoint_ns\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m \u001b[39mwith\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcursor(transaction\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m) \u001b[39mas\u001b[39;49;00m cur:\n\u001b[0;32m    214\u001b[0m     \u001b[39m# find the latest checkpoint for the thread_id\u001b[39;49;00m\n\u001b[0;32m    215\u001b[0m     \u001b[39mif\u001b[39;49;00m checkpoint_id \u001b[39m:=\u001b[39;49m get_checkpoint_id(config):\n\u001b[0;32m    216\u001b[0m         cur\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    217\u001b[0m             \u001b[39m\"\u001b[39;49m\u001b[39mSELECT thread_id, checkpoint_id, parent_checkpoint_id, type, checkpoint, metadata FROM checkpoints WHERE thread_id = ? AND checkpoint_ns = ? AND checkpoint_id = ?\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    218\u001b[0m             (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m             ),\n\u001b[0;32m    223\u001b[0m         )\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\checkpoint\\sqlite\\__init__.py:168\u001b[0m, in \u001b[0;36mSqliteSaver.cursor\u001b[1;34m(self, transaction)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Get a cursor for the SQLite database.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[39mThis method returns a cursor for the SQLite database. It is used internally\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39m    sqlite3.Cursor: A cursor for the SQLite database.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlock:\n\u001b[1;32m--> 168\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup()\n\u001b[0;32m    169\u001b[0m     cur \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconn\u001b[39m.\u001b[39mcursor()\n\u001b[0;32m    170\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32md:\\LLM\\DeepLearnin.ai_Agents\\llm_env_3.11\\Lib\\site-packages\\langgraph\\checkpoint\\sqlite\\__init__.py:125\u001b[0m, in \u001b[0;36mSqliteSaver.setup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_setup:\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconn\u001b[39m.\u001b[39;49mexecutescript(\n\u001b[0;32m    126\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[0;32m    127\u001b[0m \u001b[39m    PRAGMA journal_mode=WAL;\u001b[39;49;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    CREATE TABLE IF NOT EXISTS checkpoints (\u001b[39;49;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m        thread_id TEXT NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        checkpoint_ns TEXT NOT NULL DEFAULT '',\u001b[39;49;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m        checkpoint_id TEXT NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        parent_checkpoint_id TEXT,\u001b[39;49;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m        type TEXT,\u001b[39;49;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m        checkpoint BLOB,\u001b[39;49;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m        metadata BLOB,\u001b[39;49;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m        PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id)\u001b[39;49;00m\n\u001b[0;32m    137\u001b[0m \u001b[39m    );\u001b[39;49;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    CREATE TABLE IF NOT EXISTS writes (\u001b[39;49;00m\n\u001b[0;32m    139\u001b[0m \u001b[39m        thread_id TEXT NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[39m        checkpoint_ns TEXT NOT NULL DEFAULT '',\u001b[39;49;00m\n\u001b[0;32m    141\u001b[0m \u001b[39m        checkpoint_id TEXT NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    142\u001b[0m \u001b[39m        task_id TEXT NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    143\u001b[0m \u001b[39m        idx INTEGER NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    144\u001b[0m \u001b[39m        channel TEXT NOT NULL,\u001b[39;49;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m        type TEXT,\u001b[39;49;00m\n\u001b[0;32m    146\u001b[0m \u001b[39m        value BLOB,\u001b[39;49;00m\n\u001b[0;32m    147\u001b[0m \u001b[39m        PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id, task_id, idx)\u001b[39;49;00m\n\u001b[0;32m    148\u001b[0m \u001b[39m    );\u001b[39;49;00m\n\u001b[0;32m    149\u001b[0m \u001b[39m    \"\"\"\u001b[39;49;00m\n\u001b[0;32m    150\u001b[0m )\n\u001b[0;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_setup \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: Cannot operate on a closed database."
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'task': \"what is the difference between langchain and langsmith\",\n",
    "    \"max_revisions\": 2,\n",
    "    \"revision_number\": 1,\n",
    "}, thread):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
